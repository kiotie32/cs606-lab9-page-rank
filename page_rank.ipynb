{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import re\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "conf = SparkConf()\n",
    "sc = SparkContext(conf=conf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-1-984b58bf39db>:8 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-839676563f75>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\tapas data\\installation\\Anaconda\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    131\u001b[0m                     \" note this option will be removed in Spark 3.0\")\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32m~\\tapas data\\installation\\Anaconda\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    330\u001b[0m                         \u001b[1;34m\" created by %s at %s:%s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[1;32m--> 332\u001b[1;33m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[0;32m    333\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-1-984b58bf39db>:8 "
     ]
    }
   ],
   "source": [
    "\n",
    "Regression_Coefficient = 0.8\n",
    "variable1 = 40\n",
    "N = 1000\n",
    "\n",
    "\n",
    "#Algorithm Step 1\n",
    "def connecting_points(path):\n",
    "    data = sc.textFile(path)\n",
    "    return data.map(lambda line:\\\n",
    "        (int(re.split(r'\\t+', line)[0]),\\\n",
    "            int(re.split(r'\\t+', line)[1:][0])))\n",
    "\n",
    "#Algorithm Step2\n",
    "def combine(rdd):\n",
    "    return rdd.groupByKey()\\\n",
    "              .mapValues(lambda x: sorted(list(set([v for v in x]))))\\\n",
    "              .sortByKey()\n",
    "\n",
    "# Algorithm Step3\n",
    "def inv_degrees(rdd):\n",
    "    return rdd.map(lambda x: 1/len(x[1])).collect()\n",
    "\n",
    "# Graph\n",
    "def M(rdd, inv_d):\n",
    "    return rdd.flatMapValues(lambda x: x)\\\n",
    "              .map(lambda x: (x[1], x[0]))\\\n",
    "              .groupByKey()\\\n",
    "              .mapValues(lambda x: [(v, inv_d[v-1]) for v in x])\\\n",
    "              .sortByKey()\n",
    "\n",
    "# Initialize r\n",
    "def initialize_r():\n",
    "    return [1/N]*N\n",
    "conf = SparkConf()\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "\n",
    "Regression_Coefficient = 0.8\n",
    "variable1 = 40\n",
    "N = 1000\n",
    "\n",
    "\n",
    "#Algorithm Step 1\n",
    "def connecting_points(path):\n",
    "    data = sc.textFile(path)\n",
    "    return data.map(lambda line:\\\n",
    "        (int(re.split(r'\\t+', line)[0]),\\\n",
    "            int(re.split(r'\\t+', line)[1:][0])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine same edges\n",
    "def combine(rdd):\n",
    "    return rdd.groupByKey()\\\n",
    "              .mapValues(lambda x: sorted(list(set([v for v in x]))))\\\n",
    "              .sortByKey()\n",
    "\n",
    "# Obtain outgoing degrees\n",
    "def inv_degrees(rdd):\n",
    "    return rdd.map(lambda x: 1/len(x[1])).collect()\n",
    "\n",
    "# Graph\n",
    "def M(rdd, inv_d):\n",
    "    return rdd.flatMapValues(lambda x: x)\\\n",
    "              .map(lambda x: (x[1], x[0]))\\\n",
    "              .groupByKey()\\\n",
    "              .mapValues(lambda x: [(v, inv_d[v-1]) for v in x])\\\n",
    "              .sortByKey()\n",
    "\n",
    "# Initialize r\n",
    "def initialize_r():\n",
    "    return [1/N]*N\n",
    "\n",
    "# PageRank\n",
    "def pagerank(r, m):\n",
    "    return m.mapValues(lambda x: sum([r[v[0]-1]*v[1]*BETA for v in x]))\\\n",
    "            .mapValues(lambda x: x+(1-BETA)/N)\\\n",
    "            .map(lambda lines: lines[1]).collect()\n",
    "\n",
    "# Iterate\n",
    "def iterate(r, m):\n",
    "    for j in range(MAX_ITER):\n",
    "        r = pagerank(r, m)\n",
    "    return r\n",
    "\n",
    "# Find the top and bottom nodes\n",
    "def top_and_bottom(r):\n",
    "    r_sorted = sorted(r)\n",
    "    r = np.array(r)\n",
    "    bottom = []\n",
    "    top = []\n",
    "    for j in range(5):\n",
    "        bottom.append((np.where(r==r_sorted[j])[0][0]+1, r_sorted[j]))\n",
    "        top.append((np.where(r==r_sorted[-j-1])[0][0]+1, r_sorted[-j-1]))\n",
    "    return top, bottom\n",
    "\n",
    "# Print results\n",
    "def print_results(top, bottom):\n",
    "    print('------iteration 1------')\n",
    "    print('Top:')\n",
    "    for j in range(5):\n",
    "        print('id: '+ str(top[j][0]) + ', score: '+str(top[j][1]))\n",
    "    print('\\nBottom:')\n",
    "    for j in range(5):\n",
    "        print('id: '+ str(bottom[j][0]) + ', score: '+str(bottom[j][1]))\n",
    "\n",
    "# Solution\n",
    "def iterate_1(path):\n",
    "    edges = load_edges(path)\n",
    "    edges = combine(edges)\n",
    "    inv_d = inv_degrees(edges)\n",
    "    graph = M(edges, inv_d)\n",
    "    r = initialize_r()\n",
    "    r = iterate(r, graph)\n",
    "    top, bottom = top_and_bottom(r)\n",
    "    print_results(top, bottom)\n",
    "\n",
    "# Initialize h\n",
    "def initialize_h():\n",
    "    return [1]*N\n",
    "\n",
    "# Link matrix\n",
    "def LT(rdd):\n",
    "    return rdd.flatMapValues(lambda x: x)\\\n",
    "              .map(lambda x: (x[1], x[0]))\\\n",
    "              .groupByKey()\\\n",
    "              .mapValues(lambda x: [v for v in x])\\\n",
    "              .sortByKey()\n",
    "\n",
    "# Compute a\n",
    "def A(h, lt):\n",
    "    return lt.mapValues(lambda x: sum([h[v-1] for v in x]))\\\n",
    "             .map(lambda lines: lines[1]).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute h\n",
    "def H(a, l):\n",
    "    return l.mapValues(lambda x: sum([a[v-1] for v in x]))\\\n",
    "            .map(lambda lines: lines[1]).collect()\n",
    "\n",
    "# Iterate\n",
    "def iterate_b(h,l,lt):\n",
    "    for j in range(MAX_ITER):\n",
    "        a = A(h, lt)\n",
    "        a_max = max(a)\n",
    "        for j in range(len(a)): a[j] /= a_max\n",
    "        h = H(a, l)\n",
    "        h_max = max(h)\n",
    "        for j in range(len(h)): h[j] /= h_max\n",
    "    return a, h\n",
    "\n",
    "# Print results\n",
    "def print_results_b(top_a, bottom_a, top_h, bottom_h):\n",
    "    print('\\n------iteration2------')\n",
    "    print('Top Hubbiness:')\n",
    "    for j in range(5):\n",
    "        print('id: '+ str(top_h[j][0]) + ', score: '+str(top_h[j][1]))\n",
    "    print('\\nBottom Hubbiness:')\n",
    "    for j in range(5):\n",
    "    for j in range(5):\n",
    "        print('id: '+ str(bottom_h[j][0]) + ', score: '+str(bottom_h[j][1]))\n",
    "    print('\\nTop Authority:')\n",
    "    for j in range(5):\n",
    "        print('id: '+ str(top_a[j][0]) + ', score: '+str(top_a[j][1]))\n",
    "    print('\\nBottom Authority:')\n",
    "    for j in range(5):\n",
    "        print('id: '+ str(bottom_a[j][0]) + ', score: '+str(bottom_a[j][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Solution for b\n",
    "def iterate_2(path):\n",
    "    edges = load_edges(path)\n",
    "    l = combine(edges)\n",
    "    lt = LT(l)\n",
    "    h = initialize_h()\n",
    "    a, h = iterate_b(h,l,lt)\n",
    "    top_a, bottom_a = top_and_bottom(a)\n",
    "    top_h, bottom_h = top_and_bottom(h)\n",
    "    print_results_b(top_a, bottom_a, top_h, bottom_h)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#    iterate_1(\"data/graph-full.txt\")\n",
    "    iterate_2(r\"cs606-lab9-pagerank\\output\\small.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
